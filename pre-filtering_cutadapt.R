# DADA2 for trimming off primers and merging the reads
library(dplyr)
library(tidyr)
library(tibble)
library(ShortRead)
library(dada2)
library(Biostrings)
library(stats)

# fastq files are local, in the Documents folder, --> miseq_data
path <- file.path("miseq_data/hotel_pilot_1")
countFastq(path)

#ShortRead QA option
#a QA report is generated by creating a vector of paths to FASTQ files
files <- dir(path, "*fastq", full=TRUE)
files
qaSummary <- qa(files, type = "fastq")
typeof(qaSummary)
browseURL(report(qaSummary))

# list of files
list.files(path)
fnFs <- sort(list.files(path, pattern = "R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "R2_001.fastq", full.names = TRUE))
print(fnFs)
print(fnRs)
# The primers!
# with the adapters
# uF <- "ACACTCTTTCCCTACACGACGCTCTTCCGATCTNNNNNNGTCGGTAAAACTCGTGCCAGC"
# uR <- "GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCTNNNNNNCATAGTGGGGTATCTAATCCCAGTTTG"
# --> this pair results in counts:
#                  Forward Complement Reverse RevComp
# FWD.ForwardReads       0          0       0       0
# FWD.ReverseReads       0          0       0       0
# REV.ForwardReads       0          0       0       0
# REV.ReverseReads       0          0       0       0
# All 0 because miseq already trims off the adapters. 

# without the adapters
FWD <- "GTCGGTAAAACTCGTGCCAGC"
REV <- "CATAGTGGGGTATCTAATCCCAGTTTG"
#                  Forward Complement Reverse RevComp
# FWD.ForwardReads  183541          0       0       0
# FWD.ReverseReads       0          0       0       6
# REV.ForwardReads       0          0       0      66
# REV.ReverseReads  146372          0       0       0

# u2F <- "ACACTCTTTCCCTACACGACGCTCTTCCGATCTNNNNNNGCCGGTAAAACTCGTGCCAGC"
# u2R <- "GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCTNNNNNNCATAGGAGGGTGTCTAATCCCCGTTTG"
# eF <- "ACACTCTTTCCCTACACGACGCTCTTCCGATCTNNNNNNRGTTGGTAAATCTCGTGCCAGC"
# eR <- "GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCTNNNNNNGCATAGTGGGGTATCTAATCCTAGTTTG"

# Pre-filtering to remove those with ambigious bases
# Pre-filtered files put in folder filtN, file names end with filtN
# Primer mapping is performed on these pre-filtered files for accurate short-read mapping
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

# Function to get all possible orientations of these primers
allOrients <- function(primer) {
  # Create all orientations of the input sequence
  require(Biostrings)
  dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
FWD.orients
# Forward              Complement                 Reverse                 RevComp 
# "GTCGGTAAAACTCGTGCCAGC" "CAGCCATTTTGAGCACGGTCG" "CGACCGTGCTCAAAATGGCTG" "GCTGGCACGAGTTTTACCGAC" 

REV.orients <- allOrients(REV)
REV.orients
# Forward                    Complement                       Reverse                       RevComp 
# "CATAGTGGGGTATCTAATCCCAGTTTG" "GTATCACCCCATAGATTAGGGTCAAAC" "GTTTGACCCTAATCTATGGGGTGATAC" "CAAACTGGGATTAGATACCCCACTATG" 

# counts the number of times the primers appear in the fw and rev read
# considers all possible primer orientations
primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}

# applied only to the first file in the filtN, same mapping assumed for all samples prepared with the same method for a seq 
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
#                  Forward Complement Reverse RevComp
# FWD.ForwardReads  183541          0       0       0
# FWD.ReverseReads       0          0       0       6
# REV.ForwardReads       0          0       0      66 
# REV.ReverseReads  146372          0       0       0

# Now, we selectively remove primers
# cutadapt: 
cutadapt <- "C:/Users/Ayse-Oshima/AppData/Roaming/Python/Python39/Scripts"
system2(cutadapt, args = "--version") #R has found cutadapt

# We now create output filenames for the cutadapt-ed files, 
# and define the parameters we are going to give the cutadapt command. 
# The critical parameters are the primers, and they need to be in the right orientation, 
# i.e. the FWD primer should have been matching the forward-reads in its forward orientation, 
# and the REV primer should have been matching the reverse-reads in its forward orientation. 
# Warning: A lot of output will be written to the screen by cutadapt!
# path.cut <- file.path(path, "cutadapt")
# if(!dir.exists(path.cut)) dir.create(path.cut)
# fnFs.cut <- file.path(path.cut, basename(fnFs))
# fnFs.cut
# fnRs.cut <- file.path(path.cut, basename(fnRs))
# fnRs.cut

# FWD.RC <- dada2:::rc(FWD)
# REV.RC <- dada2:::rc(REV)
# # Trim FWD and the reverse-complement of REV off of R1 (forward reads)
# R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# R1.flags
# # Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
# R2.flags <- paste("-G", REV, "-A", FWD.RC)
# R2.flags
#
# Run Cutadapt DOES NOT WORK ON HERE ON R, RAN ON DEIGO USING THE OUTPUT HERE
# for(i in seq_along(fnFs)) {
#   system2(cutadapt, 
#           args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
#                    "-o", fnFs.cut[i], # output file
#                    "-p", fnRs.cut[i], # second output file --> but its not outputting anything!
#                    fnFs.filtN[i], fnRs.filtN[i])) # input files
# }
# 
# # test a single run of this function
# system2(cutadapt, 
#         args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
#                  "-o", "test.fastq", # output file
#                  "-p", FILTRATION_S1_L001_R2_001.fastq, # second output file --> but its not outputting anything!
#                  fnFs.filtN[1], fnRs.filtN[1]))

# not working here for some reason, so i will do on Deigo HPC.

# R1.flags # "-g GTCGGTAAAACTCGTGCCAGC -a CAAACTGGGATTAGATACCCCACTATG"
# R2.flags    # "-g GTCGGTAAAACTCGTGCCAGC -a CAAACTGGGATTAGATACCCCACTATG"
# fnFs.cut[1]

path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnFs.cut
fnRs.cut <- file.path(path.cut, basename(fnRs))
fnRs.cut

fnRs.cut[[1]]
fnRs.cut[1]

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))
# after adding the -m 1 command:
# Forward Complement Reverse RevComp
# FWD.ForwardReads       0          0       0       0
# FWD.ReverseReads       0          0       0       0
# REV.ForwardReads       0          0       0       0
# REV.ReverseReads       0          0       0       0
# All primers removed successfully
# The primer-free sequence files are now ready to be analyzed through the DADA2 pipeline. 

cutFs <- sort(list.files(path.cut, pattern = "R1_001.fastq", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "R2_001.fastq", full.names = TRUE))
cutFs
cutRs

get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)
sample.names
# The quality profile plot is a gray-scale heatmap of the frequency of 
# each quality score at each base position. 
# The median quality score at each position is shown by the green line, 
# and the quartiles of the quality score distribution by the orange lines. 
# The read line shows the scaled proportion of reads that extend to at least that position.

plotQualityProfile(cutFs[1:2])
plotQualityProfile(cutRs[1:2])

# Filter and Trim 
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))
filtFs

out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, 
                     maxN = 0, maxEE = c(2, 2), 
                     truncQ = 2, minLen = 50, rm.phix = TRUE, 
                     compress = TRUE, multithread = FALSE)  # on windows, set multithread = FALSE
head(out)
out
# reads.in reads.out
# ANA2-KAPA_S14_L001_R1_001.fastq     188150    184972
# ANA2-Q5_S13_L001_R1_001.fastq       203253    200526
# C-EXTRA_S5_L001_R1_001.fastq        142215    139682
# C-FIELD_S7_L001_R1_001.fastq          9115      1544
# C-POOL_S6_L001_R1_001.fastq         134085    130489
# C1_S3_L001_R1_001.fastq              88829     85589
# C3_S4_L001_R1_001.fastq              95540     92279
# D-EXTRA_S10_L001_R1_001.fastq       132113    129832
# D-FIELD_S12_L001_R1_001.fastq         8756       455
# D-POOL_S11_L001_R1_001.fastq        183279    179999
# D1_S8_L001_R1_001.fastq              87986     86515
# D3_S9_L001_R1_001.fastq             111963    108917
# EXTRACTION_S2_L001_R1_001.fastq      10608      1875
# FILTRATION_S1_L001_R1_001.fastq      83488     75402
# Undetermined_S0_L001_R1_001.fastq    58890     51812

errF <- learnErrors(filtFs, multithread=FALSE)
# 102978806 total bases in 835081 reads from 7 samples will be used for learning the error rates.
errR <- learnErrors(filtRs, multithread=FALSE)
# 114429293 total bases in 964913 reads from 8 samples will be used for learning the error rates.
plotErrors(errF, nominalQ = TRUE)
plotErrors(errR, nominalQ = TRUE)

derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names

# Sample inference
dadaFs <- dada(derepFs, err = errF, multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, multithread = TRUE)

# Merge paired reads
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)

# Construct Sequence Table, ASV table!
seqtab <- makeSequenceTable(mergers)
dim(seqtab) # 15 412  

# Remove chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
# Identified 273 bimeras out of 412 input sequences.
dim(seqtab.nochim) # 15 139 --> 14 more compared to the previous truncLen trimming 15 125
View(seqtab.nochim)

table(nchar(getSequences(seqtab.nochim)))
# 58 149 155 167 168 169 170 171 172 173 174 178 
# 1   1   1  17  27  21  42  12  10   5   1   1 
sum(seqtab.nochim)/sum(seqtab)
# 0.9746605

write.csv(seqtab.nochim, "plots/seqtab_nochim.csv")

seqtab.nochim <- read.csv("plots/seqtab_nochim.csv", header = TRUE, row.names = 1)
View(seqtab.nochim)

asv_table <- as.data.frame(t(as.matrix(seqtab.nochim))) %>%
  rownames_to_column("seq")
View(asv_table)

# take seq column and put into new table, add #ID of ASV1, ASV2, ASV3 ...
asv_seqs <- asv_table["seq"]
asv_seqs
# View(asv_seqs)
asv_ids <- data.frame(paste0(rep("ASV", nrow(asv_col)), 1:nrow(asv_col)))
colnames(asv_ids) <- "#ID"
# View(asv_ids)
asv_map <- cbind(asv_ids, asv_seqs)
asv_map
View(asv_map)

write.csv(asv_map, "plots/asv_map.csv")

write.table(asv_map, "plots/asv_map", sep = "\t",
            quote = FALSE,
            row.names = FALSE, col.names = TRUE,
            append = FALSE)

# asv_map <- read.csv("asv_map.csv", header = TRUE)
# View(asv_map)

asv_table_mapped <- inner_join(asv_map, asv_table, by = "seq") %>%
  select(-c("seq"))
View(asv_table_mapped)

# needs to be tab-delimited (with ext .tab) for the LCA script
write.table(asv_table_mapped, file = "plots/asv_table_mapped.tab", sep = "\t",
            quote = FALSE, # important!
            row.names = FALSE, col.names = TRUE, 
            append = FALSE)

# also need to create a fasta file of: 
# >ASV1
# ATGC.....

# with BioStrings
asv_fasta <- DNAStringSet(asv_map$seq)
names(asv_fasta) <- asv_map$`#ID`
writeXStringSet(asv_fasta, filepath = "asv_table.fasta", format = "fasta")

#####################################################################################
# We now inspect the the number of reads that made it through each step in the pipeline to verify everything worked as expected.
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, 
                                                                       getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace
# sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", 
                     "nonchim")
rownames(track) <- sample.names
head(track)
track
#               input filtered denoisedF denoisedR merged nonchim
# ANA2-KAPA    188150   184972    184807    182611 180737  177059
# ANA2-Q5      203253   200526    200268    199745 197131  167106
# C-EXTRA      142215   139682    139655    139554 138765  138196
# C-FIELD        9115     1544      1464      1260   1217    1110
# C-POOL       134085   130489    130435    130395 129865  129691
# C1            88829    85589     85528     85450  85165   85165
# C3            95540    92279     92258     92225  92083   92083
# D-EXTRA      132113   129832    129763    129412 128786  128634
# D-FIELD        8756      455       385       328    305     305 
# D-POOL       183279   179999    179898    179357 178345  177942
# D1            87986    86515     86480     86364  85614   85432
# D3           111963   108917    108875    107871 107286  106904
# EXTRACTION    10608     1875      1864      1843   1720    1720
# FILTRATION    83488    75402     75367     75213  75187   75067
# Undetermined  58890    51812     51269     49087  44505   43638
write.csv(track, "plots/filtering_results.csv")

#####





