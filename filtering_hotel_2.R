# DADA2 for trimming off primers and merging the reads
library(dplyr)
library(tidyr)
library(tibble)
library(ShortRead)
library(dada2)
library(Biostrings)
library(stats)

# fastq files are local, in the Documents folder, --> miseq_data
path <- file.path("data/hotel_pilot_2")
path.output <- file.path(path, "output")
path.output
countFastq(path)

#ShortRead QA option
#a QA report is generated by creating a vector of paths to FASTQ files
files <- dir(path, "*fastq", full=TRUE)
files
qaSummary <- qa(files, type = "fastq")
browseURL(report(qaSummary))

# list of files
list.files(path)
fnFs <- sort(list.files(path, pattern = "R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "R2_001.fastq", full.names = TRUE))
print(fnFs)
print(fnRs)

# MiFish primers
FWD <- "GTCGGTAAAACTCGTGCCAGC"
REV <- "CATAGTGGGGTATCTAATCCCAGTTTG"

# Pre-filtering to remove reads with ambigious bases
# Pre-filtered files put in folder filtN, file names end with filtN
# Primer mapping is performed on these pre-filtered files for accurate short-read mapping
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

# Function to get all possible orientations of these primers
allOrients <- function(primer) {
  # Create all orientations of the input sequence
  require(Biostrings)
  dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
# FWD.orients
# Forward              Complement                 Reverse                 RevComp 
# "GTCGGTAAAACTCGTGCCAGC" "CAGCCATTTTGAGCACGGTCG" "CGACCGTGCTCAAAATGGCTG" "GCTGGCACGAGTTTTACCGAC" 

REV.orients <- allOrients(REV)
# REV.orients
# Forward                    Complement                       Reverse                       RevComp 
# "CATAGTGGGGTATCTAATCCCAGTTTG" "GTATCACCCCATAGATTAGGGTCAAAC" "GTTTGACCCTAATCTATGGGGTGATAC" "CAAACTGGGATTAGATACCCCACTATG" 

# counts the number of times the primers appear in the fw and rev read
# considers all possible primer orientations
primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}

# applied only to the first file in the filtN, same mapping assumed for all samples prepared with the same method for a seq 
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))

#                  Forward Complement Reverse RevComp
# FWD.ForwardReads   89401          0       0       0
# FWD.ReverseReads       0          0       0     100
# REV.ForwardReads       0          0       0     745
# REV.ReverseReads   88992          0       0       0

# Now, we selectively remove primers with cutadapt:
cutadapt <- "C:/Users/Ayse-Oshima/AppData/Roaming/Python/Python39/Scripts/cutadapt.exe"
system2(cutadapt, args = "--version") #R has found cutadapt

# We now create output filenames for the cutadapt-ed files, 
# and define the parameters we are going to give the cutadapt command. 
# The critical parameters are the primers, and they need to be in the right orientation, 
# i.e. the FWD primer should have been matching the forward-reads in its forward orientation, 
# and the REV primer should have been matching the reverse-reads in its forward orientation. 
# Warning: A lot of output will be written to the screen by cutadapt!
path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnFs.cut
fnRs.cut <- file.path(path.cut, basename(fnRs))
fnRs.cut

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC)
R1.flags
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC)
R2.flags
#
# 
for(i in seq_along(fnFs)) {
  system2(cutadapt,
          args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                   "-o", fnFs.cut[i], # output file
                   "-p", fnRs.cut[i], # second output file --> but its not outputting anything!
                   fnFs.filtN[i], fnRs.filtN[i])) # input files
}

# test a single run of this function
system2(cutadapt,
        args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                 "-o", "test.fastq", # output file
                 "-p", FILTRATION_S1_L001_R2_001.fastq, # second output file --> but its not outputting anything!
                 fnFs.filtN[1], fnRs.filtN[1]))

# If running on deigo instead, run this loop command:
###################################################################
# for fwd in *_L001_R1_001.fastq;
# do base=$(basename $fwd _L001_R1_001.fastq);
# rev=${base}_L001_R2_001.fastq;
# output_fwd=/flash/RavasiU/Ayse/hotel_pilot/cutadapt/A_B/$fwd;
# output_rev=/flash/RavasiU/Ayse/hotel_pilot/cutadapt/A_B/$rev;
# 
# cutadapt \
# -G CATAGTGGGGTATCTAATCCCAGTTTG -A GCTGGCACGAGTTTTACCGAC \
# -g GTCGGTAAAACTCGTGCCAGC -a CAAACTGGGATTAGATACCCCACTATG \
# -n 2 -m 1 \
# -o $output_fwd -p $output_rev \
# $fwd $rev ;
# done
###################################################################

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))
# after adding the -m 1 command:
# Forward Complement Reverse RevComp
# FWD.ForwardReads       0          0       0       0
# FWD.ReverseReads       0          0       0       0
# REV.ForwardReads       0          0       0       0
# REV.ReverseReads       0          0       0       0
# All primers removed successfully
# The primer-free sequence files are now ready to be analyzed through the DADA2 pipeline. 

cutFs <- sort(list.files(path.cut, pattern = "R1_001.fastq", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "R2_001.fastq", full.names = TRUE))
cutFs
cutRs

get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
# head(sample.names)
sample.names
# The quality profile plot is a gray-scale heatmap of the frequency of 
# each quality score at each base position. 
# The median quality score at each position is shown by the green line, 
# and the quartiles of the quality score distribution by the orange lines. 
# The read line shows the scaled proportion of reads that extend to at least that position.

plotQualityProfile(cutFs[1:2])
plotQualityProfile(cutRs[1:2])

# Filter and Trim 
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))
filtFs

out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, 
                     maxN = 0, maxEE = c(2, 2), 
                     truncQ = 2, minLen = 50, rm.phix = TRUE, 
                     compress = TRUE, multithread = FALSE)  # on windows, set multithread = FALSE
head(out)
out

#                                      reads.in reads.out
# A-POOL_S4_L001_R1_001.fastq.gz          92492     90864
# A1_S1_L001_R1_001.fastq.gz              62987     61841
# A2-Q5_S9_L001_R1_001.fastq.gz           49349     47339
# A2_S2_L001_R1_001.fastq.gz              71819     70103
# A3-Q5_S10_L001_R1_001.fastq.gz          43740     41442
# A3_S3_L001_R1_001.fastq.gz              79731     78431
# B-POOL_S8_L001_R1_001.fastq.gz         102518    101065
# B1-Q5_S11_L001_R1_001.fastq.gz          73315     71973
# B1_S5_L001_R1_001.fastq.gz              90986     90009
# B2_S6_L001_R1_001.fastq.gz              80684     78472
# B3_S7_L001_R1_001.fastq.gz              95369     94193
# Undetermined_S0_L001_R1_001.fastq.gz    19979     18275

errF <- learnErrors(filtFs, multithread=FALSE)
# 102978806 total bases in 835081 reads from 7 samples will be used for learning the error rates.
errR <- learnErrors(filtRs, multithread=FALSE)
# 114429293 total bases in 964913 reads from 8 samples will be used for learning the error rates.
plotErrors(errF, nominalQ = TRUE)
plotErrors(errR, nominalQ = TRUE)

derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names

# Sample inference
dadaFs <- dada(derepFs, err = errF, multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, multithread = TRUE)

# Merge paired reads
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)

# Construct Sequence Table, ASV table!
seqtab <- makeSequenceTable(mergers)
dim(seqtab) # [1]  12 150

# Remove chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
# Identified 37 bimeras out of 150 input sequences.
dim(seqtab.nochim) # 12 113 
View(seqtab.nochim)

table(nchar(getSequences(seqtab.nochim)))
# 74 128 167 168 169 170 171 172 173 175 
# 1   1   8  24  25  31  11   6   2   4 
sum(seqtab.nochim)/sum(seqtab)
# 0.9968868, less than 1 % were chimeric. good.

# write it if for the first time, read it in if not.
path.seqtab.nochim <- file.path(path.output, "seqtab_nochim.csv")
write.csv(seqtab.nochim, path.seqtab.nochim)
# seqtab.nochim <- read.csv("plots/seqtab_nochim.csv", header = TRUE, row.names = 1)
View(seqtab.nochim)

# transpose the seqtab.nochim as a dataframe so it is in the acceptable format for downstream
asv_table <- as.data.frame(t(as.matrix(seqtab.nochim))) %>%
  rownames_to_column("seq")
View(asv_table)

# make ASV_map 
# take seq column and put into new table, add #ID of ASV1, ASV2, ASV3 ...
asv_seqs <- asv_table["seq"]
asv_seqs
# View(asv_seqs)
asv_ids <- data.frame(paste0(rep("ASV", nrow(asv_col)), 1:nrow(asv_col)))
colnames(asv_ids) <- "#ID"
# View(asv_ids)
asv_map <- cbind(asv_ids, asv_seqs)
asv_map
View(asv_map)

# saved both as csv and tab
path.asv.map.csv <- file.path(path.output, "asv_map.csv")
write.csv(asv_map, "path.asv.map")

path.asv.map.table <- file.path(path.output, "asv_map")
write.table(asv_map, path.asv.map.table, sep = "\t",
            quote = FALSE,
            row.names = FALSE, col.names = TRUE,
            append = FALSE)

asv_table_mapped <- inner_join(asv_map, asv_table, by = "seq") %>%
  select(-c("seq"))
View(asv_table_mapped)

path.asv.table.mapped.tab <- file.path(path.output, "asv_table_mapped.tab")
path.asv.table.mapped.tab
# needs to be tab-delimited (with ext .tab) for the LCA script
write.table(asv_table_mapped, file = path.asv.table.mapped.tab, sep = "\t",
            quote = FALSE, # important!
            row.names = FALSE, col.names = TRUE, 
            append = FALSE)

# also need to create a fasta file of: 
# >ASV1
# ATGC.....
# >ASV2
# ATGC.....

# with BioStrings
path.asv.fasta <- file.path(path.output, "asv_table.fasta")
asv_fasta <- DNAStringSet(asv_map$seq)
names(asv_fasta) <- asv_map$`#ID`
writeXStringSet(asv_fasta, filepath = , format = "fasta")

#####################################################################################
# We now inspect the the number of reads that made it through each step in the pipeline to verify everything worked as expected.
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, 
                                                                       getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace
# sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", 
                     "nonchim")
rownames(track) <- sample.names
head(track)
track
#               input filtered denoisedF denoisedR merged nonchim
# ANA2-KAPA    188150   184972    184807    182611 180737  177059
# ANA2-Q5      203253   200526    200268    199745 197131  167106
# C-EXTRA      142215   139682    139655    139554 138765  138196
# C-FIELD        9115     1544      1464      1260   1217    1110
# C-POOL       134085   130489    130435    130395 129865  129691
# C1            88829    85589     85528     85450  85165   85165
# C3            95540    92279     92258     92225  92083   92083
# D-EXTRA      132113   129832    129763    129412 128786  128634
# D-FIELD        8756      455       385       328    305     305 
# D-POOL       183279   179999    179898    179357 178345  177942
# D1            87986    86515     86480     86364  85614   85432
# D3           111963   108917    108875    107871 107286  106904
# EXTRACTION    10608     1875      1864      1843   1720    1720
# FILTRATION    83488    75402     75367     75213  75187   75067
# Undetermined  58890    51812     51269     49087  44505   43638
path.filter.results <- file.path(path.output, "filtering_results.csv")
write.csv(track, path.filter.results)

#####

